{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple inspiré de la vidéo : https://www.youtube.com/watch?v=Uz3B9fVb4LQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system\n",
    "import sys\n",
    "import time\n",
    "#math\n",
    "import numpy as np\n",
    "import sympy as sy\n",
    "#data\n",
    "import pandas as pd\n",
    "#vis\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. $f$, $\\nabla f$ and $\\nabla^2 f$ definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section enables to express f (objective function), its gradient vector and hessian matrix symbolically and evaluate them at specific coordinates.\n",
    "To do so, express analititically f by indexing the symbolic variable x. For instance x[0] represent first dimension, x[1] the second and so on.\n",
    "grad_f_exp and hess_f_exp are only there to automatically differentiate f_exp. To evaluate the f, grad_f or hess_f functions, call them precising the desirated coordinates.\n",
    "This evaluation use lambdify function and uses \"numpy\" format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : sympy.IndexedBase\n",
    "        symbolic variable managing indexes. For instance, instead of declaring n variable for the n dimensions (x, y, z, ...) which isn't easily scalable, \n",
    "        here variables are automaticly indexed (x_1, x_2, ..., x_n).\n",
    "\n",
    "    xk : np.array (float)\n",
    "        Coordinates to evaluate the function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.float (scalar or numpy.array)\n",
    "        Depending on the considered function, return a scalar, a (n, ) vector or a (n, n) array.\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The use of specific function to express f, grad_f and hess_f is to improve scalability in the case of not having analitic expression.\n",
    "    For instance, when only have a black box model which evaluate f, grad_f (and hess_f). \n",
    "\n",
    "    References\n",
    "    ------\n",
    "    https://docs.sympy.org/latest/index.html\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>x = sy.IndexedBase('x')\n",
    "    >>>xk = [-4, 3]\n",
    "    >>>dk = grad_f(x, xk)\n",
    "    \"\"\"\n",
    "\n",
    "def f_exp(x):\n",
    "    #return 3*x[0]**2 + 2*x[1]**2 + 20*sy.cos(x[0])*sy.cos(x[1])+40\n",
    "    return .26*(x[0]**2 + x[1]**2) - .48*x[0]*x[1]\n",
    "\n",
    "def f(x, xk):\n",
    "    return sy.lambdify(x, f_exp(x), \"numpy\")(xk)\n",
    "\n",
    "def grad_f_exp(x, xk):\n",
    "    return [sy.diff(f_exp(x), x[i]) for i in range(len(xk))]\n",
    "\n",
    "def grad_f(x, xk):\n",
    "    lambdify = [sy.lambdify(x, gf, \"numpy\") for gf in grad_f_exp(x, xk)]\n",
    "    return np.array([lambdify[i](xk) for i in range(len(xk))])\n",
    "\n",
    "def hess_f_exp(x, xk):\n",
    "    return [[sy.diff(g, x[i]) for i in range(len(xk))] for g in grad_f_exp(x, xk)]\n",
    "\n",
    "def hess_f(x, xk):\n",
    "    lambdify = [[sy.lambdify(x, gf, \"numpy\") for gf in Hs] for Hs in hess_f_exp(x, xk)]\n",
    "    return np.array([[lambdify[i][j](xk) for i in range(len(xk))] for j in range(len(xk))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Armijo rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Armijo():\n",
    "    \"\"\"Armijo is a class implementing the Armijo's rule using a backtracking method. It has 3 functions in addition to the __init__ function.\n",
    "    - fit(): is the main function, ensuring the convergency and implementing the backtracking algorithm. It calls phi(), theta() \n",
    "    - phi(): is the function to evaluate the first terme of Armijo's rule. It calls the function f() previously defined by the user.\n",
    "    - theta(): is the function to evaluate the second terme of Armijo's rule. It calls the functions f() and grad_f() previously defined by the user.\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The class respect a commun pattern for all line search classes. To evaluate descent step call the function fit().\n",
    "\n",
    "    References\n",
    "    ------\n",
    "    J. F. Bonnans, J. C. Gilbert, C. Lemaréchal, C. A. Sagastizábal, 2006. Numerical Optimization: Theoretical and Practical Aspects. https://doi.org/10.5860/choice.41-0357\n",
    "    https://github.com/scikit-learn/scikit-learn\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>armijolinesearch = Armijo(x, xk, dk, a0=a0, b=b, w_1=w_1, max_it=max_it)\n",
    "    >>>m, alpha = armijolinesearch.fit()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        x,\n",
    "        xk,\n",
    "        dk,\n",
    "        *,\n",
    "        w_1=.015,\n",
    "        a0=15,\n",
    "        b=.95,\n",
    "        max_it=50\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : sympy.IndexedBase\n",
    "            Previously defined, needed to evaluate phi() and theta()\n",
    "        xk : np.array (float)\n",
    "            Coordinates to evaluate the function\n",
    "        dk : np.array (float)\n",
    "            direction descent at iteration k\n",
    "        w_1 : float, optional\n",
    "            hyperparameter defining Armijo's condition, by default .015\n",
    "        a0 : int, optional\n",
    "            hyperparameter defining initial guess of a (biggest), by default 15\n",
    "        b : float, optional\n",
    "            hyperparameter defining the backtracking geometric convergency, by default .95\n",
    "        max_it : int, optional\n",
    "            hyperparameter defining maximum of iteration, by default 50\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.xk = xk\n",
    "        self.dk = dk\n",
    "        self.a0 = a0\n",
    "        self.b = b\n",
    "        self.w_1 = w_1\n",
    "        self.max_it = max_it\n",
    "\n",
    "    def phi(Armijo, m):\n",
    "        phi = f(Armijo.x, Armijo.xk + Armijo.b**m * Armijo.a0 * Armijo.dk)\n",
    "        return phi\n",
    "\n",
    "    def theta(Armijo, m):\n",
    "        theta = f(Armijo.x, Armijo.xk) + Armijo.w_1 * Armijo.b**m * Armijo.a0 * np.dot(grad_f(Armijo.x, Armijo.xk), Armijo.dk)\n",
    "        return theta\n",
    "    \n",
    "    def fit(Armijo):\n",
    "        \"\"\"fit Run until max iteration is reached (not converged) or a respecting Armijo's rule step is found.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list(int, float): [m, a]\n",
    "            Returns both iteration steps needed and a respecting Armijo's rule step.\n",
    "        \"\"\"\n",
    "        m = 0\n",
    "        while(m <= Armijo.max_it):\n",
    "            if Armijo.phi(m) <= Armijo.theta(m):\n",
    "                break\n",
    "            else:\n",
    "                m += 1\n",
    "\n",
    "            \"\"\"if(m==Armijo.max_it):\n",
    "                print(m)\n",
    "                print('a didn\\'t converged')\"\"\"\n",
    "\n",
    "        a = Armijo.a0 * Armijo.b**(m)\n",
    "        return [m, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wolfe():\n",
    "    \"\"\"Wolfe is a class implementing the Wolfe's conditions using a bissection method. It has 4 functions in addition to the __init__ function.\n",
    "    - fit(): is the main function, ensuring the convergency and implementing the bissection algorithm. It calls phi(), phi_grad() and theta() \n",
    "    - phi(): is the function to evaluate the first terme of Wolfe's condition. It calls the function f() previously defined by the user.\n",
    "    - phi_grad(): is the function to evaluate the first terme of Wolfe's condition. It calls the function grad_f() previously defined by the user.\n",
    "    - theta(): is the function to evaluate the second terme of Wolfe's condition. It calls the functions f() and grad_f() previously defined by the user.\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The class respect a commun pattern for all line search classes. To evaluate descent step call the function fit().\n",
    "\n",
    "    References\n",
    "    ------\n",
    "    J. F. Bonnans, J. C. Gilbert, C. Lemaréchal, C. A. Sagastizábal, 2006. Numerical Optimization: Theoretical and Practical Aspects. https://doi.org/10.5860/choice.41-0357\n",
    "    https://github.com/scikit-learn/scikit-learn\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>goldsteinlinesearch = Goldstein(x, xk, dk, w_1=w_1, w_2=w_2, a_max=a_max, max_it=max_it)\n",
    "    >>>it, a = goldsteinlinesearch.fit()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        x,\n",
    "        xk,\n",
    "        dk,\n",
    "        *,\n",
    "        w_1=.15,\n",
    "        w_2=.5,\n",
    "        a_max=15,\n",
    "        max_it=50\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : sympy.IndexedBase\n",
    "            Previously defined, needed to evaluate phi() and theta()\n",
    "        xk : np.array (float)\n",
    "            Coordinates to evaluate the function\n",
    "        dk : np.array (float)\n",
    "            direction descent at iteration k\n",
    "        w_1 : float, optional\n",
    "            hyperparameter defining Wolfe's first condition, by default .15\n",
    "        w_2 : float, optional\n",
    "            hyperparameter defining Wolfe's second condition, by default .5\n",
    "        a_max : int, optional\n",
    "            hyperparameter defining maximum step size, by default 15\n",
    "        max_it : int, optional\n",
    "            hyperparameter defining maximum of iteration, by default 50\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.xk = xk\n",
    "        self.dk = dk\n",
    "        self.w_1 = w_1\n",
    "        self.w_2 = w_2\n",
    "        self.a_r = a_max\n",
    "        self.a_l = 0\n",
    "        self.max_it = max_it\n",
    "        \n",
    "    def phi(Wolfe, a):\n",
    "        phi = f(Wolfe.x, Wolfe.xk + a * Wolfe.dk)\n",
    "        return phi\n",
    "    \n",
    "    def phi_grad(Wolfe, a):\n",
    "        phi_grad = np.dot(grad_f(Wolfe.x, Wolfe.xk + a*Wolfe.dk), Wolfe.dk)\n",
    "        return phi_grad\n",
    "\n",
    "    def theta(Wolfe, a):\n",
    "        theta = f(Wolfe.x, Wolfe.xk) + Wolfe.w_1 * a * np.dot(grad_f(Wolfe.x, Wolfe.xk), Wolfe.dk)\n",
    "        return theta\n",
    "\n",
    "    def fit(Wolfe):\n",
    "        \"\"\"\n",
    "        Calculate a the mean of a window ([a_l, a_r]) and check if c1 is met. If not, shrink the window from the right.\n",
    "        Then check if c2 is met, if so break, if not, depending on the value (positive or negative of phi_grad(a)) shrink the window from the right or the left.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list(int, float): [it, a]\n",
    "            Returns both iteration steps needed and a respecting Goldstein's rule step.\n",
    "\n",
    "        \"\"\"\n",
    "        it = 0\n",
    "        c1 = Wolfe.phi(Wolfe.a_l)\n",
    "        c2 = Wolfe.w_2*Wolfe.phi_grad(0)\n",
    "        \n",
    "        while(it <= Wolfe.max_it):\n",
    "            a = (Wolfe.a_l + Wolfe.a_r)/2\n",
    "            if Wolfe.phi(a) > Wolfe.theta(a) or Wolfe.phi(a) >= c1:\n",
    "                Wolfe.a_r = a\n",
    "            else:\n",
    "                if np.abs(Wolfe.phi_grad(a)) <= - c2:\n",
    "                    print(f'stop it: {it + 1}')\n",
    "                    break\n",
    "                else:\n",
    "                    if (Wolfe.phi_grad(a) > 0):\n",
    "                        Wolfe.a_r = a       \n",
    "                    else:\n",
    "                        Wolfe.a_l = a\n",
    "                        c1 = Wolfe.phi(Wolfe.a_l)\n",
    "            it += 1\n",
    "            if(it==Wolfe.max_it):\n",
    "                print(it)\n",
    "                print('a n\\'a pas convergé')\n",
    "        return [it, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Goldstein():\n",
    "    \"\"\"Goldstein is a class implementing the Goldstein's conditions using a bissection method. It has 4 functions in addition to the __init__ function.\n",
    "    - fit(): is the main function, ensuring the convergency and implementing the bissection algorithm. It calls phi(), phi_grad() and theta() \n",
    "    - phi(): is the function to evaluate the first terme of Goldstein's rule. It calls the function f() previously defined by the user.\n",
    "    - phi_grad(): is the function to evaluate the first terme of Goldstein's rule. It calls the function grad_f() previously defined by the user. (only used to calculate phi'(0))\n",
    "    - theta(): is the function to evaluate the second terme of Goldstein's rule. It calls the functions f() and grad_f() previously defined by the user.\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The class respect a commun pattern for all line search classes. To evaluate descent step call the function fit().\n",
    "\n",
    "    References\n",
    "    ------\n",
    "    J. F. Bonnans, J. C. Gilbert, C. Lemaréchal, C. A. Sagastizábal, 2006. Numerical Optimization: Theoretical and Practical Aspects. https://doi.org/10.5860/choice.41-0357\n",
    "    https://github.com/scikit-learn/scikit-learn\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>goldsteinlinesearch = Goldstein(x, xk, dk, w_1=w_1, w_2=w_2, a_max=a_max, max_it=max_it)\n",
    "    >>>it, a = goldsteinlinesearch.fit()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        x,\n",
    "        xk,\n",
    "        dk,\n",
    "        *,\n",
    "        w_1=.15,\n",
    "        w_2=.5,\n",
    "        a_max=15,\n",
    "        max_it=50\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : sympy.IndexedBase\n",
    "            Previously defined, needed to evaluate phi() and theta()\n",
    "        xk : np.array (float)\n",
    "            Coordinates to evaluate the function\n",
    "        dk : np.array (float)\n",
    "            direction descent at iteration k\n",
    "        w_1 : float, optional\n",
    "            hyperparameter defining Goldstein's first condition, by default .15\n",
    "        w_2 : float, optional\n",
    "            hyperparameter defining Goldstein's second condition, by default .5\n",
    "        a_max : int, optional\n",
    "            hyperparameter defining maximum step size, by default 15\n",
    "        max_it : int, optional\n",
    "            hyperparameter defining maximum of iteration, by default 50\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.xk = xk\n",
    "        self.dk = dk\n",
    "        self.w_1 = w_1\n",
    "        self.w_2 = w_2\n",
    "        self.a_r = a_max\n",
    "        self.a_l = 0\n",
    "        self.max_it = max_it\n",
    "        \n",
    "    def phi(Goldstein, a):\n",
    "        phi = f(Goldstein.x, Goldstein.xk + a * Goldstein.dk)\n",
    "        return phi\n",
    "    \n",
    "    def phi_grad(Goldstein, a):\n",
    "        phi_grad = np.dot(grad_f(Goldstein.x, Goldstein.xk + a*Goldstein.dk), Goldstein.dk)\n",
    "        return phi_grad\n",
    "\n",
    "    def theta(Goldstein, a):\n",
    "        theta = f(Goldstein.x, Goldstein.xk) + Goldstein.w_1 * a * np.dot(grad_f(Goldstein.x, Goldstein.xk), Goldstein.dk)\n",
    "        return theta\n",
    "\n",
    "    def fit(Goldstein):\n",
    "        \"\"\"\n",
    "        Calculate a the mean of a window ([a_l, a_r]) and check if c1 is met. If not, shrink the window from the right.\n",
    "        Then check if c2 is met, if so break, if not, shrink the window from the left.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list(int, float): [it, a]\n",
    "            Returns both iteration steps needed and a respecting Goldstein's rule step.\n",
    "\n",
    "        \"\"\"\n",
    "        it = 0\n",
    "        c2 = Goldstein.w_2*Goldstein.phi_grad(0)\n",
    "        phi0 = Goldstein.phi(0)\n",
    "\n",
    "        while(it <= Goldstein.max_it):\n",
    "            a = (Goldstein.a_l + Goldstein.a_r)/2\n",
    "            if Goldstein.phi(a) > Goldstein.theta(a):\n",
    "                Goldstein.a_r = a\n",
    "            else:\n",
    "                if (Goldstein.phi(a) - phi0)/a >= c2:\n",
    "                    print(f'stop it: {it + 1}')\n",
    "                    break\n",
    "                else:\n",
    "                    Goldstein.a_l = a\n",
    "            it += 1\n",
    "            if(it==Goldstein.max_it):\n",
    "                print('a n\\'a pas convergé')\n",
    "        return [it, a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Newton():\n",
    "    \"\"\"Newton is a class implementing the Newton-Raphson algorithm using a adaptative learning rate. It has 4 functions in addition to the __init__ function.\n",
    "    - fit(): is the main function, ensuring the convergency and implementing the Newton method.\n",
    "    - stop_pos(), stop_grad() and stop_func() are respectively criteria for stopping on position, gradient and function. \n",
    "    Using absolute norm to compare all values to epsillon (hyperparameter).\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The class respect a commun pattern for all gradient descent classes. To evaluate descent step call the function fit().\n",
    "\n",
    "    References\n",
    "    ------\n",
    "    J. F. Bonnans, J. C. Gilbert, C. Lemaréchal, C. A. Sagastizábal, 2006. Numerical Optimization: Theoretical and Practical Aspects. https://doi.org/10.5860/choice.41-0357\n",
    "    https://github.com/scikit-learn/scikit-learn\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>Newton = Newton(x, a, x0)\n",
    "    >>>xk, dk, fk, ak = Newton.fit()\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        x,\n",
    "        a,\n",
    "        x0,\n",
    "        *,\n",
    "        stop='gradient',                 #critère d'arrêt sur le gradient, la fonction objectif ou x\n",
    "        epsilon=.001,\n",
    "        max_it=50,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : sympy.IndexedBase\n",
    "            symbolic variable managing indexes\n",
    "        a : class\n",
    "            hyperparameter calling desired line search method\n",
    "        x0 : list(float)\n",
    "            hyperparameter defining initial point for gradient descent\n",
    "        b : int, optional\n",
    "            hyperparameter defining learning rate damping factor, by default 1\n",
    "        stop : str, optional\n",
    "            hyperparameter defining stopping criteria (\"gradient\", \"position\", \"function\"), by default 'gradient'\n",
    "        epsilon : float, optional\n",
    "            hyperparameter defining stopping criteria threshold, by default .001\n",
    "        max_it : int, optional\n",
    "            hyperparameter defining maximum of iteration, by default 50\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a = a\n",
    "        self.x0 = x0\n",
    "        self.stop = stop\n",
    "        self.epsilon = epsilon\n",
    "        self.max_it = max_it\n",
    "        \n",
    "        #init\n",
    "        self.xk = np.array([x0])\n",
    "        self.fk = np.array([f(x, x0)])\n",
    "        self.dk = np.array([[np.inf, np.inf]])      #Init at np.inf to ensure not to converge at iteration 1\n",
    "        self.ak = np.array([])\n",
    "        \n",
    "\n",
    "    def stop_pos(Newton):\n",
    "       if (np.abs(Newton.xk[-1]-Newton.xk[-2]) <= Newton.epsilon).all():\n",
    "          print(f'xk converged: {np.abs(Newton.xk[-1]-Newton.xk[-2])} < {Newton.epsilon}')\n",
    "          return True\n",
    "            \n",
    "    def stop_grad(Newton):\n",
    "        if (np.abs(Newton.dk[-1]-Newton.dk[-2]) <= Newton.epsilon).all():\n",
    "          print(f'grad converged: {np.abs(Newton.dk[-1]-Newton.dk[-2])} < {Newton.epsilon}')\n",
    "          return True\n",
    "        \n",
    "    def stop_func(Newton):\n",
    "        if (np.abs(Newton.fk[-1]-Newton.fk[-2]) <= Newton.epsilon).all():\n",
    "          print(f'func converged: {np.abs(Newton.fk[-1]-Newton.fk[-2])} < {Newton.epsilon}')\n",
    "          return True\n",
    "\n",
    "    def fit(Newton):\n",
    "        it=0\n",
    "        stop_criterion = {\"position\": Newton.stop_pos, \n",
    "                            \"gradient\": Newton.stop_grad, \n",
    "                            \"function\": Newton.stop_func}\n",
    "        \n",
    "        #init\n",
    "        #compute descent direction\n",
    "        dk = - np.dot(np.linalg.inv(hess_f(Newton.x, Newton.xk[-1])), grad_f(Newton.x, Newton.xk[-1]))\n",
    "        #append normalized direction\n",
    "        Newton.dk = np.append(Newton.dk, [dk/np.linalg.norm(dk)], axis=0)\n",
    "        #compute new step size\n",
    "        Newton.ak = np.append(Newton.ak, [Newton.a(Newton.x, Newton.xk[-1], Newton.dk[-1]).fit()[-1]])       #[-1] to get only alpha and not number of it\n",
    "        #update new point\n",
    "        Newton.xk = np.append(Newton.xk, [Newton.x0 + Newton.ak[-1]*Newton.dk[-1]], axis=0)\n",
    "        #evaluate objectif function\n",
    "        Newton.fk = np.append(Newton.fk, [f(Newton.x, Newton.xk[-1])], axis=0)\n",
    "        \n",
    "        while (it <= Newton.max_it):\n",
    "            dk = - np.dot(np.linalg.inv(hess_f(Newton.x, Newton.xk[-1])), grad_f(Newton.x, Newton.xk[-1]))\n",
    "            Newton.dk = np.append(Newton.dk, [dk/np.linalg.norm(dk)], axis=0)\n",
    "            Newton.ak = np.append(Newton.ak, [Newton.a(Newton.x, Newton.xk[-1], Newton.dk[-1]).fit()[-1]])\n",
    "            Newton.xk = np.append(Newton.xk, [Newton.xk[-1] + Newton.ak[-1]*Newton.dk[-1]], axis=0)\n",
    "            Newton.fk = np.append(Newton.fk, [f(Newton.x, Newton.xk[-1])], axis=0)\n",
    "\n",
    "            #convergency test\n",
    "            if stop_criterion[Newton.stop]():\n",
    "                print(f'converged at it: {it}')\n",
    "                break\n",
    "            \n",
    "            it += 1\n",
    "        print(f'not converged')\n",
    "        return [Newton.xk, Newton.dk, Newton.fk, Newton.ak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop it: 1\n",
      "stop it: 3\n",
      "stop it: 6\n",
      "stop it: 9\n",
      "stop it: 14\n",
      "xk converged: [0.00078506 0.00047103] < 0.001\n",
      "converged at it: 3\n",
      "not converged\n",
      "t: 0.2721700668334961 s\n",
      "x*: [ 3.58067595e-05 -2.14840557e-05]\n",
      "grad*: [ 0.85749293 -0.51449576]\n",
      "f*: 8.226107757733654e-10\n"
     ]
    }
   ],
   "source": [
    "#symbolic variable definition\n",
    "x = sy.IndexedBase('x')\n",
    "\n",
    "#Steepest_descent_optimal hyperparameters\n",
    "a = Wolfe\n",
    "x0 = [-5, 3]\n",
    "lim = [[-5, 5], [-5, 5]]\n",
    "\n",
    "#Steepest_descent_optimal fit\n",
    "start = time.time()\n",
    "Newton = Newton(x, a, x0, stop='position')\n",
    "xk, dk, fk, ak = Newton.fit()\n",
    "\n",
    "#print output\n",
    "print(f\"t: {time.time() - start} s\")\n",
    "print(f\"x*: {xk[-1]}\")\n",
    "print(f\"grad*: {dk[-1]}\")\n",
    "print(f\"f*: {fk[-1]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(lim, x, xk, fk, x_idx=0, y_idx=1):\n",
    "    \"\"\"visualization is a function, using plotly to represente dynamicaly the optimization iterations steps on f. Pandas df are used to facilitate plotly usage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lim : list(n, 2)\n",
    "        limite du domaine d'optimisation pour chachune des dimensions\n",
    "    x : sympy.IndexedBase\n",
    "        symbolic variable managing indexes.\n",
    "    xk : nd array(float)\n",
    "        array of xk iterations\n",
    "    fk : nd array(float)\n",
    "        array of evaluation of f at xk iterations\n",
    "    x_idx : int, optional\n",
    "        index of the first dimension, by default 0\n",
    "    y_idx : int, optional\n",
    "        index of the second dimension, by default 1\n",
    "\n",
    "    References\n",
    "    ------\n",
    "    https://plotly.com/python/\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>x_idx = 0\n",
    "    >>>y_idx = 1\n",
    "    >>>visualization(lim, x, xk, fk, x_idx, y_idx)\n",
    "    \"\"\"\n",
    "    if x_idx >= np.shape(xk)[1] or y_idx >= np.shape(xk)[1]:\n",
    "        print('---------- Dimension out of bound, please choose right dimensions ----------')\n",
    "        sys.exit()\n",
    "    #Surface plot\n",
    "    X = np.linspace(lim[0][0], lim[0][1], 100)\n",
    "    Y = np.linspace(lim[1][0], lim[1][1], 100)\n",
    "    mesh_X, mesh_Y = np.meshgrid(X, Y)\n",
    "    Z = f(x,[mesh_X,mesh_Y])\n",
    "\n",
    "    #gradient descent plot\n",
    "    grad_desc = pd.DataFrame(xk, columns=['x', 'y'])\n",
    "    grad_desc['z'] = np.reshape(fk, (-1,1))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_surface(x=X, y=Y, z=Z)\n",
    "    fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                    highlightcolor=\"limegreen\", project_z=True))\n",
    "    fig.update_layout(title='Optimization surface', autosize=False,\n",
    "                    width=750, height=750\n",
    "    )\n",
    "    \n",
    "    #iterations plot\n",
    "    fig.add_scatter3d(\n",
    "        x=grad_desc[grad_desc.columns[x_idx]],\n",
    "        y=grad_desc[grad_desc.columns[y_idx]],\n",
    "        z=grad_desc['z'],\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=grad_desc['x'].index,\n",
    "            colorscale='hot',\n",
    "        ),\n",
    "        line=dict(\n",
    "            color='white',\n",
    "            width=3\n",
    "        ))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
