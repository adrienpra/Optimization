{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! Underconstruction !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. $f$, $\\nabla f$ and $\\nabla^2 f$ definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section enables to express f (objective function), its gradient vector and hessian matrix symbolically and evaluate them at specific coordinates.\n",
    "To do so, express analititically f by indexing the symbolic variable x. For instance x[0] represent first dimension, x[1] the second and so on.\n",
    "grad_f_exp and hess_f_exp are only there to automatically differentiate f_exp. To evaluate the f, grad_f or hess_f functions, call them precising the desirated coordinates.\n",
    "This evaluation use lambdify function and uses \"numpy\" format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : sympy.IndexedBase\n",
    "        symbolic variable managing indexes. For instance, instead of declaring n variable for the n dimensions (x, y, z, ...) which isn't easily scalable, \n",
    "        here variables are automaticly indexed (x_1, x_2, ..., x_n).\n",
    "\n",
    "    xk : np.array (float)\n",
    "        Coordinates to evaluate the function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.float (scalar or numpy.array)\n",
    "        Depending on the considered function, return a scalar, a (n, ) vector or a (n, n) array.\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    The use of specific function to express f, grad_f and hess_f is to improve scalability in the case of not having analitic expression.\n",
    "    For instance, when only have a black box model which evaluate f, grad_f (and hess_f). \n",
    "\n",
    "    References\n",
    "    ------\n",
    "    https://docs.sympy.org/latest/index.html\n",
    "\n",
    "    Examples\n",
    "    ------\n",
    "    >>>x = sy.IndexedBase('x')\n",
    "    >>>xk = [-4, 3]\n",
    "    >>>dk = grad_f(x, xk)\n",
    "    \"\"\"\n",
    "\n",
    "def f_exp(x):\n",
    "    return 3*x[0]**2 + 2*x[1]**2 + 20*sy.cos(x[0])*sy.cos(x[1])+40\n",
    "\n",
    "def f(x, xk):\n",
    "    return sy.lambdify(x, f_exp(x), \"numpy\")(xk)\n",
    "\n",
    "def grad_f_exp(x, xk):\n",
    "    return [sy.diff(f_exp(x), x[i]) for i in range(len(xk))]\n",
    "\n",
    "def grad_f(x, xk):\n",
    "    lambdify = [sy.lambdify(x, gf, \"numpy\") for gf in grad_f_exp(x, xk)]\n",
    "    return np.array([lambdify[i](xk) for i in range(len(xk))])\n",
    "\n",
    "def hess_f_exp(x, xk):\n",
    "    return [[sy.diff(g, x[i]) for i in range(len(xk))] for g in grad_f_exp(x, xk)]\n",
    "\n",
    "def hess_f(x, xk):\n",
    "    lambdify = [[sy.lambdify(x, gf, \"numpy\") for gf in Hs] for Hs in hess_f_exp(x, xk)]\n",
    "    return np.array([[lambdify[i][j](xk) for i in range(len(xk))] for j in range(len(xk))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4.]\n",
      " [0. 1. 2. 3. 4.]\n",
      " [0. 1. 2. 3. 4.]\n",
      " [0. 1. 2. 3. 4.]]\n",
      "[[2. 2. 2. 2. 2.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [4. 4. 4. 4. 4.]\n",
      " [5. 5. 5. 5. 5.]]\n",
      "[(0.0, 2.0), (1.0, 2.0), (2.0, 2.0), (3.0, 2.0), (4.0, 2.0)]\n"
     ]
    }
   ],
   "source": [
    "dom = [[0, 4, 5], [2, 5, 4], [-3, -1, 3]]\n",
    "nx = [np.linspace(dom[i][0], dom[i][1], dom[i][2]) for i in range(len(dom))]\n",
    "X = [[[[i, j, l] for i in nx[0]] for j in nx[1]] for l in nx[2]]\n",
    "\n",
    "nxx, nyy = np.meshgrid(nx[0], nx[1])\n",
    "print(nxx)\n",
    "print(nyy)\n",
    "test = zip(nxx[0], nyy[0])\n",
    "print(list(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#[lw_bd, up_bd, nb_val]\n",
    "domaine = [[0, 4, 10], [2, 5, 10]]\n",
    "\n",
    "def X_init_uniform(domaine):\n",
    "        interval = [(dom[1] - dom[0])/dom[2] for dom in domaine]\n",
    "        X = [[domaine[j][0] + i*interval[j] for i in range(N+1)]for j in range(len(interval))]\n",
    "        return X\n",
    "\n",
    "X_init_uniform(domaine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO():\n",
    "    def __init__(\n",
    "        self,\n",
    "        f,                  #fonction coût\n",
    "        N,                  #nombre de particules de l'essaim\n",
    "        domaine,\n",
    "        *,\n",
    "        X_init=\"random\",    #plan de recherche initiale (uniforme, aléatoire)\n",
    "        stop=\"position\",    #critère d'arrêt (position ou f)        \n",
    "        seed=0,             #random_seed\n",
    "        w=.95,              #inertie vitesse\n",
    "        c1=.5,              #comportement cognitif de la particule\n",
    "        c2=.5,              #Aptitude sociale de la particule\n",
    "        max_it=50           #nombre d'itération max\n",
    "        ):\n",
    "        self.f = f\n",
    "        self.N = N\n",
    "        self.domaine = domaine\n",
    "        self.X_init = X_init\n",
    "        self.seed = seed\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.max_it = max_it\n",
    "    \n",
    "    def X_init_uniform(PSO):    #uniformément distribué sur les domaines\n",
    "        X = [(dom[0] + dom[1])/PSO.N for dom in PSO.domaine]\n",
    "        return X\n",
    "    \n",
    "    def X_init_random(PSO):     #aléatoire avec distribution uniforme\n",
    "        X = [np.random.uniform(dom, PSO.N) for dom in domaine]\n",
    "        return X\n",
    "    \n",
    "    def visualization(PSO):\n",
    "        print(\"hello\")\n",
    "\n",
    "    def fit(PSO):\n",
    "        it =0\n",
    "        r1 = np.random.rand()\n",
    "        r2 = np.random.rand()\n",
    "\n",
    "        #init = {\"uniform\": PSO.X_init_uniform(PSO), \"random\": PSO.X_init_random}\n",
    "        #X = init[PSO.X_init]\n",
    "        X = [[0, 0], [0, 1], [0, 2], [0, 3], [0, 4],\n",
    "             [1, 0], [1, 1], [1, 2], [1, 3], [1, 4]\n",
    "             [2, 0], [2, 1], [2, 2], [2, 3], [2, 4]\n",
    "             [3, 0], [3, 1], [3, 2], [3, 3], [3, 4]\n",
    "             [4, 0], [4, 1], [4, 2], [4, 3], [4, 4]]\n",
    "        x_best = X[0]\n",
    "\n",
    "        for x in X:\n",
    "            if x\n",
    "\n",
    "        G = [np.inf, ]     #set initiale global best\n",
    "        P = []\n",
    "\n",
    "        while(it <= PSO.max_it):\n",
    "            print(\"hello\")\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m: 9\n",
      "alpha: 9.453741145869136\n",
      "dk: [ 0.54568069 -0.83799319]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dans le cas d'une optimisation black box, soit on retire toutes les instances de x \n",
    "dans les fonctions, soit on le laisse et on l'utilise pas dans la définition \n",
    "de f, grad_f et hes_f\n",
    "\"\"\"\n",
    "x = sy.IndexedBase('x')\n",
    "\n",
    "a0 = 15\n",
    "b = .95\n",
    "w_1 = .15\n",
    "max_it = 500\n",
    "\n",
    "xk = [-4, 3]\n",
    "dk = - grad_f(x, xk)/np.linalg.norm(grad_f(x, xk))        #dk a unit vector\n",
    "\n",
    "armijolinesearch = Armijo(x, xk, dk, a0=a0, b=b, w_1=w_1, max_it=max_it)\n",
    "m, alpha = armijolinesearch.fit()\n",
    "print(f\"m: {m}\")\n",
    "print(f\"alpha: {alpha}\")\n",
    "print(f\"dk: {dk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
